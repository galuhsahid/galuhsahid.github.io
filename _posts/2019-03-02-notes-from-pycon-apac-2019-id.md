---
layout: post
title:  "Catatan dari PyCon APAC 2019"
date:   2019-03-02
tags: [conference, translation]
---

Beberapa minggu yang lalu, saya datang ke PyCon APAC 2019 di Manila. PyCon APAC ini adalah PyCon kedua saya, & kali ini saya hadir sebagai peserta. Di *post* ini, saya akan menulis tentang *talk* yang saya tonton serta apa saja yang saya pelajari di konferensi. Ada dua *track* & saya kebanyakan berada di *data track*, sehingga kebanyakan dari *talk* yang saya akan bahas adalah *talk* yang ada di *data track*.

# Talks
## Keynote 1: PyCon APAC - Back to the Future! by Beng Keat Liew
Di *keynote* pertama ini, Dr Liew Beng Keat membahas tentang evolusi dari komunitas Python di daerah Asia Pasifik. Beliaulah yang membangun Python Singapore User Group serta menginisiasi PyCon APAC sepuluh tahun yang lalu. Mendengar hal tersebut, saya langsung berpikir: 10 tahun yang lalu saya ada di mana, ya? Sepertinya saya lagi nongkrong di kamar main Stardoll.com. :P

Saya sendiri sebenarnya baru mengenal komunitas Python, & bahkan sampai sekarang saya juga masih jarang datang ke *meetup* atau konferensi, hanya kadang-kadang saja. Jadi, bagi saya menarik sekali untuk bisa melihat bagaimana komunitas Python berevolusi dari waktu ke waktu, serta mengetahui siapa saja yang menginisiasi komunitas di masing-masing negara di Asia Pasifik. Saya sendiri percaya bahwa komunitas memiliki peran yang penting dalam memperkenalkan Python ke banyak orang, yang membuat Python menjadi bahasa pemrograman yang kita ketahui & sukai sekarang.

## Keynote 2: Interviewing as a Python engineer by Jacob Kaplan-Moss
Belakangan ini banyak sekali pembicaraan tentang proses wawancara di berbagai perusahaan teknologi, baik itu di Twitter maupun di antara teman-teman saya. Sehingga, saya rasa *talk* ini ada di waktu yang pas.

Ada banyak hal yang saya pelajari dari *talk* ini. Satu hal penting yang membantu saya dalam mehamai proses wawancara adalah, teknik-teknik wawancara merupakan *proxy* yang membantu perusahaan-perusahaan untuk memprediksi performa kandidat di pekerjaannya nanti. Teknik terbaik adalah dengan memposisikan kandidat secara langsung di pekerjaannya, namun tentu saja cara ini tidak mungkin dilakukan, sehingga dibutuhkan teknik-teknik lain. Teknik-teknik ini tidaklah sempurna (dan hal ini juga bisa menjadi pengingat bahwa Anda tidak perlu sedih apabila ada perusahaan yang menolak Anda!). Beberapa teknik ini merupakan prediktor yang baik, sebagian kadang bekerja dengan baik, & beberapa merupakan prediktor yang buruk meskipun banyak perusahaan yang masih mengimplementasikan teknik tersebut. Satu hal menarik yang disebutkan adalah: Anda bisa menyimpulkan budaya dari sebuah perusahaan dari tipe-tipe wawancara yang mereka pilih untuk proses perekrutan---apabila mereka memilih teknik yang buruk, maka Anda bisa simpulkan bahwa mereka tidak memprioritaskan proses perekrutan yang efektif. :)

Jacob Kaplan-Moss membagi tipe-tipe teknik wawancara ke dalam tiga kategori: **question/answer**, **coding challenges**, & **exercises**.

Di kategori Q/A, *behavioral interview* merupakan teknik yang baik, & biasanya *behavioral interview* dimulai dengan pertanyaan seperti "ceritain dong waktu Anda...". Syukurlah, saya belum pernah ditanyai pertanyaan yang sifatnya berupa pertanyaan trivia. Pertanyaan-pertanyaan trivia merupakan pertanyaan teknikal yang sungguh spesifik, & jawaban dari pertanyaan ini biasanya bisa ditemukan melalui pencarian di Google. Justru karena itu, kalau kita bisa langsung lihat jawabannya sendiri di dokumentasi, mengapa kita harus mengingat jawabannya di luar kepala? Pertanyaan-pertanyaan seperti ini tidak memberitahu apa-apa tentang performa seseorang di pekerjaannya, & apabila perusahaan di mana Anda melamar menanyakan pertanyaan-pertanyaan tersebut, hal ini justru mencerminkan perusahaan itu sendiri.

Untuk kategori *coding challenges*, *take-home exercises* merupakan teknik yang bagus. Saya sendiri belum pernah mengerjakan *take-home exercises* sebelumnya, namun cukup masuk akal bahwa menyelesaikan masalah di lingkungan yang lebih realistis (misalnya, tanpa ada orang yang mengamati dengan betul huruf-huruf apa sajakah yang Anda ketik) merupakan prediktor yang lebih baik dibandingkan dengan teknik seperti *whiteboarding*. *Coding challenge* yang menggunakan *online tool* seperti HackerRank juga tidak baik, karena teknik ini tidak dapat menilai bagaimana Anda berkomunikasi---Anda tidak bisa *think out loud*, tidak punya kesempatan untuk menggali lebih dalam lagi tentang permasalahannya, & teknik ini juga tidak dapat menilai kualitas dari kodemu. Yang mereka bisa nilai adalah apakah Anda lolos atau tidak sesuai dengan kompleksitas waktu atau ruang yang telah ditentukan sebelumnya.

Selanjutnya, ada *exercises*. Ada *lab exercises*, di mana Anda akan diminta untuk melakukan sebuah *task* di lingkungan yang telah diatur sebelumnya. Teknik ini merupakan teknik yang efektif, & karena persiapan teknik ini cukup membutuhkan banyak usaha, maka apabila perusahaan yang Anda lamar mengimplementasikan teknik ini, maka hal tersebut menunjukkan bahwa perusahaan tersebut serius tentang proses perekrutan yang mereka lakukan. Selain itu ada juga *starter projects*, di mana Anda mencoba untuk bekerja bersama dengan tim yang sesungguhnya. Saya sendiri belum pernah menjalani kedua teknik tersebut sehingga saya tidak bisa berbicara banyak tentang keduanya.

Tadi kita baru membicarakan beragam teknik wawancara. Nah, sekarang kita akan membahas tentang tips wawancara. Secara umum, berikut sarannya: a) persiapkan dengan baik, b) tanya pertanyaan, c) catat. *Keynote* juga membahas tentang sebuah riset yang menunjukkan bahwa mencatat di kertas jauh lebih efektif dibandingkan mencatat dengan menggunakan laptop. Yang terpenting, setelah tiap wawancara, refleksikan kembali bagaimana proses wawancara-nya berjalan. Refleksi ini dapat berguna untuk wawancara di masa mendatang. Anda juga bisa minta umpan balik meskipun terkadang perusahaan tidak akan memberikan umpan balik.

[Slides](https://jacobian.org/speaking/technical-interviews/)

## Keynote 3: How did you know? Explaining Black Box Model Predictions in Python by Suzy Lee
Lagi-lagi ini merupakan *talk* yang ada di saat yang tepat, mengingat semakin bertambahnya diskusi tentang kebutuhan untuk membuat model menjadi lebih mudah diinterpretasikan & dijelaskan. Saya juga dapat *relate* dengan *talk* ini karena permasalahan ini merupakan permasalahan yang saya hadapi dua tahun lalu, & di saat itu saya tidak begitu yakin bagaimana saya dapat menyelesaikannya. Mendapatkan nilai R-squared yang besar bukanlah akhir dari permasalahan---saat itu kami masih harus mencari cara untuk menjelaskan faktor-faktor apakah yang berkontribusi terhadap model *random forest* kami, agar kami dapat menjawab pertanyaan-pertanyaan riset yang mengawali penelitian tersebut. Sebuah angka tidaklah cukup untuk menjawab pertanyaan-pertanyaan tersebut.

Suzy Lee memulai *keynote* dengan statistik yang mencengangkan. Saya tidak mencatat berapa angka persisnya & detil sampel yang diambil, tetapi yang saya tangkap, banyak orang masih tidak mempercayakan AI untuk meng-*handle* finansial & proses perekrutan. Ini merupakan pengingat untuk orang-orang: tidak peduli betapa keren & akurat modelmu, apabila orang-orang tidak dapat mempercayai model tersebut, maka mereka tidak akan menggunakannya. 

Saya juga suka bagaimana beliau benar-benar menekankan betapa seberapa penting penjelasan black box models dengan menggunakan contoh-contoh yang konkret, karena saya rasa inilah cara yang efektif untuk membuat orang-orang sadar betapa pentingnya isu ini. Setidaknya, saya dulu tersadarkan karena contoh-contoh ini. Saya pertama kali menyadari isu ini ketika saya membaca buku *Weapons of Math Destruction* oleh Cathy O'Neil. Masing-masing bab mendiskusikan sebuah kasus, dari bagaimana *big data* memengaruhi *online advertising* sampai kans seseorang untuk memperoleh asuransi. Pada akhirnya, saya sadar akan hal-hal buruk yang dapat terjadi apabila kita tidak berusaha untuk membuat model-model ini menjadi lebih mudah untuk diinterpretasikan. Suzy Lee memberikan pengalaman personal beliau sebagai contoh---saat itu beliau sedang membuat sebuah model untuk sebuah bank, & semuanya berjalan lancar sampai ia mendapatkan pertanyaan: mengapa? Tentunya, institusi yang memiliki regulasi-regulasi yang ketat seperti bank tidak akan menerima jawaban seperti "kata modelnya sih begitu". Dan seiring dengan semakin banyaknya bagian dari hidup kita (baik kita sukai maupun tidak) yang ditentukan oleh model seperti itu, saya percaya bahwa kita semua sebagai manusia pantas untuk mendapatkan jawaban & penjelasan yang lebih baik.

Dari *talk* tersebut, saya belajar tentang  **interpretability-accuracy tradeoff**. Teknik-teknik seperti *deep learning* memang dapat meng-*handle* skenario-skenario yang lebih kompleks & dapat memberikan hasil yang lebih akurat dari, misalnya, regresi linier. Namun, *deep learning* bisa jadi lebih sulit untuk diinterpretasikan, apabila dibandingkan dengan regresi linier yang lebih mudah dijelaskan. Semakin kompleks & akurat sebuah model, maka seringkali model tersebut lebih sulit untuk diinterpretasikan.

Saya juga belajar tentang interpretasi lokal & global, & menurut saya contoh yang diberikan membantu. Misalnya, Anda sedang membuat model untuk *credit scoring*. Interpretasi lokal mencoba untuk menjelaskan variabel-variabel apakah yang membuat seseorang menjadi peminjam yang buruk untuk *instance* tersebut. Sementara itu, interpretasi global mencoba untuk menjelaskan variabel-variabel apakah yang membuat seseorang menjadi peminjam yang buruk untuk pada umumnya, bukan hanya pada *instance* tertentu saja.

Selanjutnya, *tools* apa sajakah yang dapat digunakan? Saya harap saya tahu tentang *tools* ini, karena *tools* ini bakal mengurangi sakit kepala saya dua tahun lalu. [Lime](https://github.com/marcotcr/lime), yang merupakan kependekan dari Local Interpretable Model-Agnostic Explanations, dapat menjelaskan *black box classifier* apapun dengan dua kelas atau lebih. Lime melakukan interpretasi lokal---ia akan menghasilkan *data points* bohongan di sekitar sebuah *instance* & mempelajari model linear yang mengaproksimasi model tersebut di sekitar *instance* tadi. [*Blog post*](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime) ini mengelaborasi LIME dengan lebih detil lagi.

Selain itu, ada juga [Skater](https://github.com/datascienceinc/Skater), yang berdasar dari ide bahwa apabila kita bermain-main dengan sebuah fitur & ternyata terdapat perubahan pada akurasi kita, berarti fitur tersebut berkontribusi banyak terhadap model kita. Kalau nggak, berarti fitur tersebut tidak banyak kontribusinya.

Selanjutnya, ada [Shap](https://github.com/slundberg/shap)---kependekan dari SHapley Additive exPlanations. Hal yang menarik adalah, dasar dari Shap adalah *game thoery*! Di konteks penjelasan prediksi model, permainan yang dimaksud adalah *task* prediksi itu sendiri. 

Terakhir, ada [ELI5](https://github.com/TeamHG-Memex/eli5) yang melakukan interpretasi global. ELI5 mendukung banyak *framework* & *package* pembelajaran mesin, mulai dari scikit-learn sampai XGboost.

Saya belum mempunyai kesempatan untuk bermain-main dengan *libraries* yang disebutkan di atas, tapi tentunya saya akan ketika ada kesempatannya!

## Talk: A Framework for Transfer Learning on Multi-lingual Text Data for Sentiment Classification by Gyenn Neil Ibo
Setelah bekerja dengan data berbentuk teks pada riset-riset saya sebelumnya, apabila ada yang saya sadari tentang teks informal dalam bahasa Indonesia, hal tersebut adalah: kita suka sekali mencampur-campur bahasa Indonesia & Inggris. Ternyata, fenomena ini juga banyak ditemui di Filipina. Selain itu, baik Indonesia maupun Filipina juga memiliki banyak dialek, sehingga diperlukan adanya *framework* yang dapat meng-*handle* data teks multibahasa untuk melakukan *task* seperti klasifikasi sentimen.

*Talk* ini berbicara tentang bagaimana kita dapat meng-*handle* teks data multibahasa dengan cara menggabungkan *word embedding* (misalnya, dengan menggabungkan *word embedding* bahasa Inggris & Filipino). *Word embedding* merepresentasikan data teks dalam bentuk numerik, & *word embedding* ini berdasarkan ide bahwa kata-kata yang dekat secara arti memiliki vektor yang dekat dengan satu sama lain. [Saya suka banget sama *word embedding*]((http://indonesian-word-embedding.herokuapp.com/)) sehingga saya senang banget untuk mengetahui lebih lanjut tentang penggunaan *word embedding*.

Untuk menggabungkan *word embedding*, ada dua *approach*: pertama, dengan memperolah *approximate* matriks rotasi, atau dengan menggunakan cara *hacker*. *Talk* ini berfokus pada cara *hacker* yang terdiri dari beberapa langkah: pertama, gunakan kata-kata yang ada baik di *word embedding* standar maupun *word embedding* yang kita buat sendiri. Selanjutnya, hitung vektor perpindahan relatif dari kata-kata baru atau kata-kata yang tidak ada di kosakata kita. Terakhir, proyeksikan posisi dari kata-kata yang baru pada ruang vektor dari *word embedding* yang asli.

Nah, kenapa kita ingin menggabungkan *word embedding*? Pertama, kita bisa memanfaatkan *word embedding* yang sudah dilatih sebelumnya seperti word2vec & GloVe. Saya ingat waktu mengerjakan skripsi, saya melatih *word embedding* saya sendiri. Namun, data yang saya gunakan terlalu kecil untuk menghasilkan *word embedding* yang bagus. Andai saja waktu itu saya kepikiran untuk menggabungkan *word embedding* yang saya buat dengan word2vec yang telah dilatih sebelumnya...

Poin menarik lainnya adalah: bahasa terus berevolusi, & ini benar banget khususnya kalau kita bekerja dengan data teks yang informal seperti teks-teks yang ada di media sosial. Dengan menggabungkan *word embedding*, di saat yang sama kita juga dapat meng-*update* *word embedding* standar. Selain itu, dengan menggabungkan *word embedding*, kita juga dapat memanfaatkan data yang dilabel oleh VADER sebagai data pelatihan.

Nah, saya sendiri belum banyak bermain-main dengan VADER sehingga saya nggak bisa ngomong banyak tentang itu, tapi inilah yang saya tahu. Singkatnya, Valence Aware Dictionary and sEntiment Reasoner (VADER) adalah alat analisis sentimen yang berbasis leksikon & aturan. Anda bisa lihat leksikon-nya [di sini](https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt). Leksikon ini dianotasi oleh manusia (dari [Mechanical Turk milik Amazon](https://www.mturk.com/)). Nah yang keren tentang VADER adalah kita nggak butuh data pelatihan, karena semuanya berbasis leksikon & aturan. Bahkan, kita bisa menggunakan hasil yang kita dapat dari VADER untuk melatih *word embedding* kita sendiri yang nantinya akan digabung dengan, misal, *word embedding* GloVe. *Word embedding* yang telah digabung ini bisa digunakan sebagai lapis fitur awal untuk model *neural network* yang akan kita gunakan untuk klasifikasi sentimen nanti. 

Lalu, apa selanjutnya? Kita masih membutuhkan cara yang lebih akademis---ingatlah, bahwa cara ini adalah cara *hacker*. :) Mungkin kita perlu mengganti metode yang awalnya masih menggunakan perpindahan relatif untuk proyeksi menjadi metode optimisasi.

## Keynote 4: Here Come The Robots - Python and Machine Learning by Tom Dyson
*Keynote* ini memberikan gambaran besar yang baik tentang apa sih yang bisa dilakukan oleh pembelajaran mesin saat ini, & bagaimana kita bisa membuat aplikasi pembelajaran mesin hanya dengan beberapa baris saja menggunakan Python. *Keynote* ini tidak terlalu detil membahas tentang teknisnya, tapi nggak apa-apa. Di *keynote* ini banyak banget demo-nya, & saya sendiri senang melihat masing-masing demo yang ditunjukkan. Saya rasa orang lain juga sama senangnya dengan saya, & semoga demo-demo tersebut dapat menginspirasi banyak orang untuk mulai belajar tentang pembelajaran mesin.

Contohnya, ada situs [This Person Does Not Exist](https://thispersondoesnotexist.com/) yang menghasilkan foto dari seseorang yang sebenarnya nggak nyata ketika kita *refresh* halamannya. Foto-foto ini diproduksi oleh *generative adversarial network* (GAN). Situs ini sebenarnya baru dirilis beberapa hari sebelum konferensi. Meski melihat foto-foto yang dibuat oleh GAN merupakan suatu hal yang menyenangkan, kita juga perlu untuk mengetahui bagaimana caranya kita bisa mengidentifikasi gambar yang palsu. Saya senang *keynote* ini juga membahas hal-hal yang perlu kita perhatikan terkait pembelajaran mesin.

Tom Dyson juga menjelaskan bagaimana perusahaannya dapat memanfaatkan pembelajaran mesin untuk sebuah yayasan berbasis di Britania Raya & Irlandia bernama [Samaritans](https://www.samaritans.org/), yang bertujuan untuk memberikan dukungan emosional kepada siapapun yang sedang menghadapi pikiran-pikiran yang menyedihkan atau sedang menghadapi resiko bunuh diri. Selain itu, ada banyak contoh dari aplikasi yang praktikal seperti ekstraksi entitas & prediksi.

Diskusi tentang bagaimana pembelajaran mesin digunakan hari ini nggak akan lengkap tanpa membahas pertanyaan: apa selanjutnya? Saya senang bahwa jawaban dari pertanyaan ini bukanlah "akurasi yang lebih baik!", melainkan:
- Mengurangi kompleksitas (ingat *keynote* Suzy Lee di hari pertama---model yang lebih kompleks biasanya lebih susah diinterpretasikan!)
- Mengurangi biaya (saat ini kita masih membutuhkan biaya yang besar kalau mau mencapai hasil yang sangat bagus)
- Selain *comprehension*, juga *generation* (sempat dibahas juga tentang [model bahasa OpenAI, GPT-2, yang baru-baru ini marak dibahas](https://blog.openai.com/better-language-models/))
- *ML at the edge*

[Slides](https://s3.amazonaws.com/tom/Here-Come-The-Robots-PyCon-APAC.pdf) & [demo](https://robots.now.sh)

## Talk: Using Artificial Intelligence and Satellite Imagery to Zero In on the Philippinesâ€™ Most Vulnerable Communities
Menggunakan AI untuk menyelesaikan isu-isu sosial di Indonesia adalah salah satu tujuan hidup saya, sehingga saya sangat antusias sekali untuk menonton *talk* ini.

Seperti di Indonesia, kemiskinan juga merupakan masalah besar di Filipina. Pemerintah memang memiliki data terkait kemiskinan---misalnya, kita bisa tahu bahwa Cebu merupakan salah satu provinsi terkaya di Filipina. Namun, ketika kita lihat lebih detil lagi, kita akan melihat bahwa Cebu memiliki area-area tertentu dengan tingkat kemiskinan yang tinggi. Kita tidak bisa melihat detil ini di level provinsi. Hal ini merupakan masalah, karena biasanya data terkait pemasukan rumah tangga dikukpulkan melalui survei secara manual, & dilaporkan di tingkat regional/provinsi saja. Terlebih lagi, metode ini sungguhlah mahal & jarang dilakukan (hanya sekali di 3-5 tahun!). Sehingga, kita memerlukan cara yang lebih cepat & murah untuk mengukur kemiskinan di Filipina. Nggak hanya itu, kita juga memerlukan hasil yang lebih granular.

Nah, jadi bagaimana caranya? Mungkin jawaban yang pertama terlintas adalah dengan menggunakan *supervised learning*. Kita bisa menggunakan gambar-gambar satelit sebagai input-nya---area-area yang lebih kaya biasanya memiliki jalan-jalan yang lebih terstruktur, atap-atap yang lebih lebar, & tidak begitu padat. Namun, di sini ada isu *data gap*. Apabila kita ingin menggunakan pendekatan ini, kita akan butuh banyak data yang sudah dilabel. Meskipun teknik-teknik *deep learning* dapat memanfaatkan gambar-gambar satelit, sayangnya tidak banyak data pelatihan untuk kasus ini.

Lalu, bagaimana caranya kita bisa menyelesaikan permasalahan ini? Kita dapat menggunakan metode yang diadopsi dari sebuah studi oleh [Jean et. al.](http://science.sciencemag.org/content/353/6301/790) dari Stanford University Sustainability and Artificial Intelligence Lab. Jawabannya adalah: *transfer learning*. Dengan *transfer learning*, kita dapat menggunakan pengetahuan yang telah kita dapat dari proses pemecahan suatu masalah, untuk memecahkan masalah lain (yang berbeda, tapi masih berkaitan!). Di kasus ini, kita dapat memulai dengan sebuah model *convolutional neural network* (CNN) yang dilatih pada ImageNet. Model ini mempelajari bagaimana ia bisa mengklasifikasikan masing-masing gambar ke 1000 kategori berbeda, & di prosesnya, model ini belajar bagaimana caranya ia bisa mengidentifikasi fitur-fitur gambar yang sifatnya *low-level* (seperti sudut-sudut, batas-batas garis, dll.).

Selanjutnya, kita dapat menggunakan pengetahuan ini untuk menyelesaikan *task* lain: melakukan prediksi terhadap intensitas lampu-lampu di malam hari, berdasarkan gambar satelit di siang hari di daerah yang sama. Idenya di sini adalah, intensitas cahaya lampu di malam hari dapat digunakan sebagai *proxy* untuk perkembangan ekonomi, karena daerah-daerah yang kaya cenderung lebih terang di malam hari. Tujuan dari *task* ini adalah untuk mengekstrak fitur-fitur yang dapat digunakan untuk memprediksi intensitas cahaya lampu ke salah satu dari tiga kelas berikut: *low*, *medium*, atau *high luminosity*. Ingat, karena cahaya lampu adalah *proxy* untuk perkembangan ekonomi, maka fitur-fitur ini secara tidak langsung dapat digunakan untuk memprediksi perkembangan ekonomi juga.

OK, sekarang masing-masing gambar satelit yang diambil di setiap hari telah memiliki vektor fitur yang merepresentasikan pola-pola yang digunakan model untuk membedakan intensitas cahaya rendah, sedang, & tinggi. Tapi kita belum selesai. Kita masih perlu mengestimasi kemiskinan itu sendiri, misalnya dengan *wealth index*. Untuk melakukannya, kita perlu menghitung nilai-nilai yang di tingkat *cluster* yang didapat dari data survei dan fitur-fitur gambar di langkah sebelumnya untuk melatih model *ridge regression* yang nantinya dapat memprediksi *wealth index*. Model akhirnya dapat menjelaskan 63% dari varians.

Karena ini adalah konferensi Python, tentunya ada diskusi tentang *tech stack* yang digunakan. :) Saya sendiri belum pernah mengolah data berbentuk gambar satelit sebelumnya. Kalau nggak salah ingat, saya pernah dengar teman-teman saya yang kuliah di jurusan geografi menggunakan QGIS. Ternyata, di Python sendiri ada *library* yang dapat digunakan bernama [geopandas](https://github.com/geopandas/geopandas). Bahkan, salah satu pembicaranya mendukung kita untuk menggunakan geopandas saja dibanding QGIS. 

Ada beberapa tantangan di proyek ini. Pertama, modelnya sendiri agak sulit untuk diinterpretasikan & saat ini mereka sedang mengusahakan agar modelnya lebih mudah untuk diinterpretasikan. Selanjutnya, data *ground truth* nya sendiri sangat terbatas, & untuk memperoleh data ini mereka sangat bergantung pada pemerintah. Meski banyak tantangan yang harus dihadapi, ada banyak potensi untuk memberikan dampak di dunia nyata. Ketika kita sudah tahu metodologi untuk memetakan kemiskinan yang cepat & skalabel, kita dapat menggunakan metodologi ini untuk banyak hal keren, seperti bantuan kemanusiaan yang terarah, kebijakan sosial yang berbasis data, serta program-program berbasis data. Proyek ini menunjukkan bahwa ada banyak kesempatan untuk menggunakan AI untuk kebaikan sosial, & saya senang banget akan hal itu.

[Blog post terkait #1](https://stories.thinkingmachin.es/philippines-most-vulnerable-communities/) & [blog post terkait #2](https://stories.thinkingmachin.es/using-transfer-learning-and-satellite-imagery-to-map-poverty-in-the-philippines)

# Keseluruhan
Nah, sekarang tentang konferensinya itu sendiri. Topik-topiknya menarik, atmosfernya juga seru, & acaranya sendiri diorganisir dengan baik. PyCon APAC tahun ini juga punya inisiatif bagus bernama *group lunches*. Saat waktu makan siang, di masing-masing meja akan ada fasilitator grup & topik yang di-*assign* ke meja tersebut. Topik-topik ini beragam, mulai dari permainan sampai komunitas. Bahkan, ada meja untuk *introvert* juga! :-D Apabila Anda tertarik dengan *board games*, misalnya, Anda dapat duduk di meja dengan topik Board Games & ngobrol dengan orang-orang yang mempunyai minat yang sama denganmu. Tanpa inisiatif ini saya kayaknya bakal makan siang sendirian. :) Selama konferensi berlangsung, saya juga bertemu dengan banyak orang hebat & ngobrol tentang banyak topik yang keren & mencerahkan.

Jadi - apa sih satu hal besar yang saya pelajari dari pengalaman saya di PyCon APAC 2019? Sebenarnya saya butuh beberapa hari untuk merefleksikan hal ini, tapi sepertinya sekarang saya tahu apa yang saya pelajari. Nggak hanya dari konferensinya saja tapi dari waktu yang saya habiskan di Manila (yang nantinya akan saya tulis di *post* terpisah). Baik Indonesia maupun Filipina mirip di banyak hal, & kita juga menghadapi berbagai isu yang cukup mirip. Seseorang yang saya temui di konferensi menyimpulkannya dengan baik: "kita semua ada di perahu yang sama", & di saat itu saya sadar: menggabungkan kekuatan & berkolaborasi antara sesama negara APAC maupun Asia Tenggara merupakan hal yang sungguh masuk akal. Saya harap saya bisa lihat lebih banyak kolaborasi tersebut di masa depan nanti, khususnya di bidang teknologi.